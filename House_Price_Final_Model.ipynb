{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "House Price Final Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMfsPM0mJia7ropTXd4I4P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villafue/Capstone_1-_Predict_House_Prices/blob/master/House_Price_Final_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuji6uGk5tOG",
        "outputId": "0bf65a46-dcc1-4a60-dbbf-86a71b7b0ef1"
      },
      "source": [
        "# Import StackingRegressor\r\n",
        "from sklearn.ensemble import StackingRegressor\r\n",
        "from sklearn.linear_model import RidgeCV\r\n",
        "\r\n",
        "dt = DecisionTreeRegressor(max_depth=3, max_features='log2')\r\n",
        "estimators = [('lr', LinearRegression()),\r\n",
        "              ('ridge', Ridge(alpha=0.03727272727272728, normalize=True, random_state=29)),\r\n",
        "              ('lasso', Lasso(alpha=0.0001, random_state=29)),\r\n",
        "              ('eln', ElasticNet(alpha=0.001, l1_ratio=0.1, random_state=29)),\r\n",
        "              ('rf', RandomForestRegressor(n_estimators=200, random_state=29)),\r\n",
        "              ('ada', AdaBoostRegressor(base_estimator=dt, n_estimators=200, learning_rate=0.20,\r\n",
        "                                        random_state=29)),\r\n",
        "              ('gbr', GradientBoostingRegressor(alpha=0.7, learning_rate=0.08, max_depth=1, \r\n",
        "                                                min_samples_split=25, min_samples_leaf=10, \r\n",
        "                                                n_estimators=2000, random_state=29)),\r\n",
        "              ('xgb', xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, \r\n",
        "                                       colsample_bynode=1, colsample_bytree=0.45, gamma=0.03, \r\n",
        "                                       importance_type='gain', learning_rate=0.01, max_delta_step=0, \r\n",
        "                                       max_depth=3, max_samples=0.2, min_child_weight=1, missing=None, \r\n",
        "                                       n_estimators=2000, n_jobs=1, nthread=None, objective='reg:squarederror', \r\n",
        "                                       random_state=29, reg_alpha=0.08, reg_lambda=0.04, scale_pos_weight=1, \r\n",
        "                                       seed=None, silent=None, subsample=0.12)) \r\n",
        "]\r\n",
        "\r\n",
        "# Instantiate StackingRegressor\r\n",
        "reg5 = StackingRegressor(estimators=estimators, verbose=2, passthrough=True, n_jobs=-1)\r\n",
        "\r\n",
        "reg5.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(reg5, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-reg5.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ----------------------------------------------------------------------------------------------------------------------------------------\n",
            "The training RMSE is: 0.1127\n",
            "The testing RMSE is: 0.1164\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}