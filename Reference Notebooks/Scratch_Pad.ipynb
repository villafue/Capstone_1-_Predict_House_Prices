{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scratch Pad.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMlbbKn7TkA+WY7xEOGTiEH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villafue/Capstone_1-_Predict_House_Prices/blob/master/Reference%20Notebooks/Scratch_Pad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrYUkOpXxxGS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjHe5Hfb6UUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba79c305-58ec-4703-fe07-991d14212e8f"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\r\n",
        "\r\n",
        "lr = LinearRegression()\r\n",
        "lr.fit(X_train, Y_train)\r\n",
        "training_results = np.sqrt((-cross_val_score(lr, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-lr.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1401\n",
            "The testing RMSE is: 0.1290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJq6Xhkxo2x-"
      },
      "source": [
        "The simple linear regression out-performed my baseline by almost 3-fold. Next, I will try a simple CART (decision tree regressor). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saGblK_ep9H-",
        "outputId": "a35617ea-58c5-4373-e9de-f8f4a2e571f8"
      },
      "source": [
        "# Import DecisionTreeRegressor from sklearn.tree\r\n",
        "from sklearn.tree import DecisionTreeRegressor\r\n",
        "\r\n",
        "dt = DecisionTreeRegressor()\r\n",
        "dt.fit(X_train, Y_train)\r\n",
        "training_results = np.sqrt((-cross_val_score(dt, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-dt.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.2060\n",
            "The testing RMSE is: 0.2077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znWhVNBcZ-WO"
      },
      "source": [
        "The simple CART did worse than my simple linear regression model. Next, I will try Random Forest Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rma34DHustIT",
        "outputId": "66e65495-7302-45dc-c424-0508f5b014ce"
      },
      "source": [
        "# Import RandomForestRegressor\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "\r\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=29)\r\n",
        "rf.fit(X_train, Y_train)\r\n",
        "training_results = np.sqrt((-cross_val_score(rf, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-rf.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1429\n",
            "The testing RMSE is: 0.1513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy4FFRvYu5ER"
      },
      "source": [
        "The powerful RandomForestRegressor did not perform as well as my simple linear model, but I'm not surprised as I did not tune any hyperparameters. It also took a long time to run.\r\n",
        "\r\n",
        "Next, I will try AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAjeVmAZ1MLq",
        "outputId": "36302a3b-6b29-4473-fdc5-c9a3537baaac"
      },
      "source": [
        "# Import AdaBoostRegressor\r\n",
        "from sklearn.ensemble import AdaBoostRegressor\r\n",
        "\r\n",
        "ar = AdaBoostRegressor(n_estimators=200, learning_rate=0.10, random_state=29)\r\n",
        "ar.fit(X_train, Y_train)\r\n",
        "training_results = np.sqrt((-cross_val_score(ar, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-ar.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1657\n",
            "The testing RMSE is: 0.1701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QxIMWH32ZuS"
      },
      "source": [
        "This performed worse than my RandomForest. Next, I will try GradientBoostRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NLDBqo0HKY3",
        "outputId": "5ecfda5b-93a1-4fe6-a6ee-4349cf72e2cb"
      },
      "source": [
        "# Import GradientBoostingRegressor\r\n",
        "from sklearn.ensemble import GradientBoostingRegressor\r\n",
        "\r\n",
        "# Instantiate gb\r\n",
        "gb = GradientBoostingRegressor(max_depth=4, learning_rate=0.1, \r\n",
        "            n_estimators=200, random_state=29)\r\n",
        "gb.fit(X_train, Y_train)\r\n",
        "training_results = np.sqrt((-cross_val_score(gb, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-gb.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1252\n",
            "The testing RMSE is: 0.1337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZLhdN1SIzZO"
      },
      "source": [
        "My Gradient Boost model performed almost as well as my linear regression model.\r\n",
        "\r\n",
        "Next, I will try a stochastic gradient boost regressor. It's similar to a normal GradientBoostingRegressor but with different hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmcrictJAiuq",
        "outputId": "b885c3ae-891d-4f4d-8607-d98322978ee2"
      },
      "source": [
        "# Import GradientBoostingRegressor\r\n",
        "from sklearn.ensemble import GradientBoostingRegressor\r\n",
        "\r\n",
        "# Instantiate sgbr\r\n",
        "sgbr = GradientBoostingRegressor(max_depth=4, \r\n",
        "            subsample=0.9,\r\n",
        "            max_features=0.75,\r\n",
        "            n_estimators=200,                                \r\n",
        "            random_state=29)\r\n",
        "\r\n",
        "sgbr.fit(X_train, Y_train)\r\n",
        "training_results = np.sqrt((-cross_val_score(sgbr, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-sgbr.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1251\n",
            "The testing RMSE is: 0.1314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sddPCPQCEAhY"
      },
      "source": [
        "It performed XXXXXXXXXXXX. Next I will try regularlized linear regression, beginning with Ridge Regression. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sgCsaL_IujY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62f3c8b-5403-4c29-bf29-7668d5a1cda2"
      },
      "source": [
        "from sklearn.linear_model import Ridge\r\n",
        "#normalize=true?\r\n",
        "Ridge = Ridge(alpha=0.10, normalize=True, random_state=29)\r\n",
        "Ridge.fit(X_train, Y_train)\r\n",
        "training_results = np.sqrt((-cross_val_score(Ridge, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-Ridge.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1209\n",
            "The testing RMSE is: 0.1219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo6qKW6yIxP8"
      },
      "source": [
        "My Ridge model has the best test RMSE so far. There is an error, however, and one potential answer can be found [here](https://stackoverflow.com/questions/58393378/why-does-ridge-model-fitting-show-warning-when-power-of-the-denominator-in-the-a).\r\n",
        "\r\n",
        "Ridge penalizes numerous features. Next, I will try Lasso which helps to reduce (and even eliminate) useless features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKzCY_vTk4Wb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a849ae-9ac1-454f-883e-d64a8acfdd2e"
      },
      "source": [
        "from sklearn.linear_model import Lasso\r\n",
        "\r\n",
        "Lasso = Lasso(alpha=0.01)\r\n",
        "Lasso.fit(X_train, Y_train)\r\n",
        "training_results = np.sqrt((-cross_val_score(Lasso, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-Lasso.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1409\n",
            "The testing RMSE is: 0.1553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnueOY1NsFZ7"
      },
      "source": [
        "Lasso did not perform as well as it's Ridge counterpart. Next, I will try ElasticNet as it's a combination of both Ridge and Lasso models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDUQOczUKrxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e64049-7e23-4dcf-a5d7-074f42e74980"
      },
      "source": [
        "from sklearn.linear_model import ElasticNet\r\n",
        "ElasticNet = ElasticNet(alpha=0.10, random_state=29)\r\n",
        "ElasticNet.fit(X_train, Y_train)\r\n",
        "training_results = np.sqrt((-cross_val_score(ElasticNet, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-ElasticNet.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1577\n",
            "The testing RMSE is: 0.1747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f06def91-4f4a-499c-9e4e-59edb14901fe",
        "_uuid": "413d60cd80f41ee1453ecde29ac7f43793241f1b",
        "trusted": true,
        "id": "QjwceZadw0-d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "8fe962b2-82ca-4ed1-955a-2439cdf53967"
      },
      "source": [
        "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
        "models = [LinearRegression(), RandomForestRegressor(n_estimators=200), AdaBoostRegressor(n_estimators=200, learning_rate=0.10), \n",
        "          GradientBoostingRegressor(max_depth=4, subsample=0.9, max_features=0.75, n_estimators=200),\n",
        "          Ridge(alpha=0.10, normalize=True), Lasso(alpha=0.01), ElasticNet(alpha=0.10), \n",
        "          xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, seed=29, verbose=1)]\n",
        "\n",
        "# First I will use ShuffleSplit as a way of randomising the cross validation samples.\n",
        "shuff = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
        "\n",
        "#create table to compare MLA metrics\n",
        "columns = ['Name', 'Parameters', 'Train Accuracy Mean', 'Test Accuracy']\n",
        "before_model_compare = pd.DataFrame(columns = columns)\n",
        "\n",
        "#index through models and save performance to table\n",
        "row_index = 0\n",
        "for alg in models:\n",
        "\n",
        "    #set name and parameters\n",
        "    model_name = alg.__class__.__name__\n",
        "    before_model_compare.loc[row_index, 'Name'] = model_name\n",
        "    before_model_compare.loc[row_index, 'Parameters'] = str(alg.get_params())\n",
        "    \n",
        "    alg.fit(X_train, Y_train)\n",
        "    \n",
        "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
        "    training_results = np.sqrt((-cross_val_score(alg, X_train, Y_train, cv = shuff, scoring= 'neg_mean_squared_error')).mean())\n",
        "    test_results = np.sqrt(((Y_test-alg.predict(X_test))**2).mean())\n",
        "    \n",
        "    before_model_compare.loc[row_index, 'Train Accuracy Mean'] = (training_results)\n",
        "    before_model_compare.loc[row_index, 'Test Accuracy'] = (test_results)\n",
        "    \n",
        "    row_index+=1\n",
        "    print(row_index, alg.__class__.__name__, 'trained...')\n",
        "\n",
        "decimals = 3\n",
        "before_model_compare['Train Accuracy Mean'] = before_model_compare['Train Accuracy Mean'].apply(lambda x: round(x, decimals))\n",
        "before_model_compare['Test Accuracy'] = before_model_compare['Test Accuracy'].apply(lambda x: round(x, decimals))\n",
        "before_model_compare"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 LinearRegression trained...\n",
            "2 RandomForestRegressor trained...\n",
            "3 AdaBoostRegressor trained...\n",
            "4 GradientBoostingRegressor trained...\n",
            "5 Ridge trained...\n",
            "6 Lasso trained...\n",
            "7 ElasticNet trained...\n",
            "8 XGBRegressor trained...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Parameters</th>\n",
              "      <th>Train Accuracy Mean</th>\n",
              "      <th>Test Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LinearRegression</td>\n",
              "      <td>{'copy_X': True, 'fit_intercept': True, 'n_job...</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RandomForestRegressor</td>\n",
              "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AdaBoostRegressor</td>\n",
              "      <td>{'base_estimator': None, 'learning_rate': 0.1,...</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>GradientBoostingRegressor</td>\n",
              "      <td>{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': ...</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ridge</td>\n",
              "      <td>{'alpha': 0.1, 'copy_X': True, 'fit_intercept'...</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Lasso</td>\n",
              "      <td>{'alpha': 0.01, 'copy_X': True, 'fit_intercept...</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ElasticNet</td>\n",
              "      <td>{'alpha': 0.1, 'copy_X': True, 'fit_intercept'...</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBRegressor</td>\n",
              "      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.123</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Name  \\\n",
              "0           LinearRegression   \n",
              "1      RandomForestRegressor   \n",
              "2          AdaBoostRegressor   \n",
              "3  GradientBoostingRegressor   \n",
              "4                      Ridge   \n",
              "5                      Lasso   \n",
              "6                 ElasticNet   \n",
              "7               XGBRegressor   \n",
              "\n",
              "                                          Parameters  Train Accuracy Mean  \\\n",
              "0  {'copy_X': True, 'fit_intercept': True, 'n_job...                0.115   \n",
              "1  {'bootstrap': True, 'ccp_alpha': 0.0, 'criteri...                0.135   \n",
              "2  {'base_estimator': None, 'learning_rate': 0.1,...                0.171   \n",
              "3  {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': ...                0.123   \n",
              "4  {'alpha': 0.1, 'copy_X': True, 'fit_intercept'...                0.115   \n",
              "5  {'alpha': 0.01, 'copy_X': True, 'fit_intercept...                0.144   \n",
              "6  {'alpha': 0.1, 'copy_X': True, 'fit_intercept'...                0.162   \n",
              "7  {'base_score': 0.5, 'booster': 'gbtree', 'cols...                0.130   \n",
              "\n",
              "   Test Accuracy  \n",
              "0          0.119  \n",
              "1          0.144  \n",
              "2          0.174  \n",
              "3          0.130  \n",
              "4          0.122  \n",
              "5          0.159  \n",
              "6          0.178  \n",
              "7          0.123  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3soGywuVOYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "928e053e-b5c7-452c-a853-a3e89ab8c7f4"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\r\n",
        "\r\n",
        "lr_int_false = LinearRegression(fit_intercept=False)\r\n",
        "lr_int_false.fit(X_train, Y_train)\r\n",
        "training_results = np.sqrt((-cross_val_score(lr_int_false, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-lr_int_false.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1149\n",
            "The testing RMSE is: 0.1194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01f1buJ2YmO9"
      },
      "source": [
        "This model did worse when setting fit_intercept=False. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONLMKjEKVN6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e6f0cc9-8212-4ee3-e57f-c323f6eeed36"
      },
      "source": [
        "lr_norm_true = LinearRegression(normalize=True)\r\n",
        "lr_norm_true.fit(X_train, Y_train)\r\n",
        "training_results = np.sqrt((-cross_val_score(lr_norm_true, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-lr_norm_true.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 26538092171.6938\n",
            "The testing RMSE is: 0.1194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqy7AlQ0ZFB2"
      },
      "source": [
        "There is an error when tuning my linear regression. The basic model is good enough.\r\n",
        "\r\n",
        "Next, I will tune Ridge. I will start with Random Search. I will set a range to randomly sample between 0.01 and 0.1. I chose this because .1 was the alpha I chose for my base Ridge model. I will sample the smaller side of it first. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_roJ3T5fXpo4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be61f8f7-1cba-497f-9213-a410e5340a97"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "\r\n",
        "# Create the parameter grid\r\n",
        "param_grid = {'alpha': np.linspace(0.01, 0.1, 100)} \r\n",
        "\r\n",
        "# Create a random search object\r\n",
        "random_Ridge = RandomizedSearchCV(\r\n",
        "    estimator = Ridge(normalize=True),\r\n",
        "    param_distributions = param_grid,\r\n",
        "    n_iter = 50,\r\n",
        "    scoring='neg_mean_squared_error', n_jobs=-1, cv = 5, refit=True, return_train_score = True, random_state=29)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "random_Ridge.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(random_Ridge, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-random_Ridge.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(random_Ridge.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1148\n",
            "The testing RMSE is: 0.1201\n",
            "Ridge(alpha=0.03727272727272728, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=True, random_state=None, solver='auto', tol=0.001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6f63YulUuE4"
      },
      "source": [
        "My tuned model performed marginally better. Next, I will sample a larger range of values above 0.1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BwBmUyzQwkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ee209a-d7b5-4782-f6cd-86591956fa9e"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "\r\n",
        "# Create the parameter grid\r\n",
        "param_grid = {'alpha': np.linspace(0.1, 100, 100)} \r\n",
        "\r\n",
        "# Create a random search object\r\n",
        "random_Ridge1 = RandomizedSearchCV(\r\n",
        "    estimator = Ridge(normalize=True, random_state=29),\r\n",
        "    param_distributions = param_grid,\r\n",
        "    n_iter = 50,\r\n",
        "    verbose=1,\r\n",
        "    scoring='neg_mean_squared_error', n_jobs=-1, cv = 5, refit=True, return_train_score = True)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "random_Ridge1.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(random_Ridge1, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-random_Ridge1.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(random_Ridge1.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1318\n",
            "The testing RMSE is: 0.1220\n",
            "Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None, normalize=True,\n",
            "      random_state=29, solver='auto', tol=0.001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1gBXKabVTqF"
      },
      "source": [
        "This sample did worse. I will now perform a GridSearch of the values below 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXWCrjNqO0K7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5695e9c6-7132-4635-878f-95378da5fc7c"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "\r\n",
        "# Create the parameter grid\r\n",
        "param_grid = {'alpha': np.linspace(0.001, .05, 100)} \r\n",
        "\r\n",
        "# Create a GridSearchCV object\r\n",
        "grid_ridge1 = GridSearchCV(\r\n",
        "    estimator=Ridge(normalize=True, random_state=29),\r\n",
        "    param_grid=param_grid,\r\n",
        "    scoring='neg_mean_squared_error',\r\n",
        "    n_jobs=-1,\r\n",
        "    verbose=1\r\n",
        "    cv=5,\r\n",
        "    refit=True, return_train_score=True)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "grid_ridge1.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(grid_ridge1, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-grid_ridge1.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(grid_ridge1.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1148\n",
            "The testing RMSE is: 0.1201\n",
            "Ridge(alpha=0.038121212121212125, copy_X=True, fit_intercept=True,\n",
            "      max_iter=None, normalize=True, random_state=29, solver='auto', tol=0.001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKA9Givmg5Ad"
      },
      "source": [
        "This is essentially the same score and same alpha level as the one identified by my random search. This is my most tuned Ridge model.\r\n",
        "\r\n",
        "Next, I will tune GradientBoostRegressor. XGBoost had the next best score, but it's going to take more time to learn the API. For now, I'll tune the other hyperparameters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-9XUzOWqdhO"
      },
      "source": [
        "I'm going to start with hyperparameters garnered from TPOT which is an automated machine learning API. These hyperparameters were part of a \"stacked\" metamodel so I don't know how great of a score they will be by themselves. However, I think it's a good baseline to work from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vMQLWHlLV10"
      },
      "source": [
        "# Average CV score on the training set was: -0.01389039370802492\r\n",
        "'''\r\n",
        "exported_pipeline = make_pipeline(\r\n",
        "    StackingEstimator(estimator=RidgeCV()),\r\n",
        "    StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.9, learning_rate=0.01, \r\n",
        "                      loss=\"quantile\", max_depth=3, max_features=0.8, min_samples_leaf=1, \r\n",
        "                      min_samples_split=2, n_estimators=100, subsample=0.8)),\r\n",
        "    VarianceThreshold(threshold=0.001),\r\n",
        "    ZeroCount(),\r\n",
        "    PCA(iterated_power=3, svd_solver=\"randomized\"),\r\n",
        "    RidgeCV()\r\n",
        ")\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfAGdyupvZ_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e14ca7fe-9fce-4ba9-d0f6-327584741a84"
      },
      "source": [
        "# Import GradientBoostingRegressor\r\n",
        "from sklearn.ensemble import GradientBoostingRegressor\r\n",
        "\r\n",
        "# Instantiate gb\r\n",
        "gb1 = GradientBoostingRegressor(alpha=0.9, learning_rate=0.01, loss=\"quantile\", max_depth=3, \r\n",
        "                               max_features=0.8, min_samples_leaf=1, min_samples_split=2, \r\n",
        "                               n_estimators=100, subsample=0.8, random_state=29)\r\n",
        "gb1.fit(X_train, Y_train)\r\n",
        "training_results = np.sqrt((-cross_val_score(gb1, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-gb1.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.4818\n",
            "The testing RMSE is: 0.5161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzt82OFEvbOo"
      },
      "source": [
        "Ok. Nevermind. This performed horribly outside the metalmodel. I will run a randomsearch. Here is a helpful link for [GradientBoost](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/) Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM_4Krj1qZgb"
      },
      "source": [
        "# Create the parameter grid\r\n",
        "param_grid = {'alpha': np.linspace(0.001, 1, 100),\r\n",
        "              'learning_rate': np.linspace(0.001, 1, 100),\r\n",
        "              'max_depth': range(1, 8),\r\n",
        "              'max_features': ('auto', 'sqrt'),\r\n",
        "              'min_samples_leaf': (1, 5, 10, 20, 30),\r\n",
        "              'min_samples_split': (1, 2, 5, 10, 20, 30, 40),\r\n",
        "              'n_estimators': (50, 100, 200, 500, 1000),\r\n",
        "              'subsample': (0.5, 0.6, 0.7, 0.8, 0.9, 1)\r\n",
        "} \r\n",
        "\r\n",
        "# Create a GridSearchCV object\r\n",
        "random_gb1 = RandomizedSearchCV(\r\n",
        "    estimator=GradientBoostingRegressor(random_state=29),\r\n",
        "    param_distributions=param_grid,\r\n",
        "    scoring='neg_mean_squared_error',\r\n",
        "    n_jobs=-1,\r\n",
        "    random_state=29,\r\n",
        "    n_iter=100,\r\n",
        "    verbose=1,\r\n",
        "    cv=5,\r\n",
        "    refit=True, return_train_score=True)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "random_gb1.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(random_gb1, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-random_gb1.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(random_gb1.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5oZ7Ax04eIO"
      },
      "source": [
        "'''\r\n",
        "This is my best model: \r\n",
        "\r\n",
        "The training RMSE is: 0.1195\r\n",
        "The testing RMSE is: 0.1262\r\n",
        "\r\n",
        "GradientBoostingRegressor(alpha=0.8587272727272728, ccp_alpha=0.0,\r\n",
        "                          criterion='friedman_mse', init=None,\r\n",
        "                          learning_rate=0.09181818181818183, loss='ls',\r\n",
        "                          max_depth=1, max_features='auto',   max_leaf_nodes=None,\r\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\r\n",
        "                          min_samples_leaf=10, min_samples_split=30,\r\n",
        "                          min_weight_fraction_leaf=0.0, n_estimators=1000,\r\n",
        "                          n_iter_no_change=None, presort='deprecated',\r\n",
        "                          random_state=None, subsample=1, tol=0.0001,\r\n",
        "                          validation_fraction=0.1, verbose=0, warm_start=False)\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDj2O1h2DIOC"
      },
      "source": [
        "Originally, I ran the above Random Search for 1000 iterations. After three hours, I had to stop it as I had to move locations. I ran it for 100 iterations, instead, and it took over 20 minutes. \r\n",
        "\r\n",
        "I will try to further tune this by using GridSearch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33QQk5-Pp9Rc"
      },
      "source": [
        "# Create the parameter grid\r\n",
        "param_grid = {'alpha': (0.7, .85872, .9),\r\n",
        "              'learning_rate': (0.08, .091818, .1),\r\n",
        "              'min_samples_split': (25, 30, 35),\r\n",
        "              'n_estimators': (1000, 1500, 2000)\r\n",
        "} \r\n",
        "\r\n",
        "# Create a GridSearchCV object\r\n",
        "grid_gb = GridSearchCV(\r\n",
        "    estimator=GradientBoostingRegressor(random_state=29, max_depth=1, min_samples_leaf=10),\r\n",
        "    param_grid=param_grid,\r\n",
        "    scoring='neg_mean_squared_error',\r\n",
        "    n_jobs=-1,\r\n",
        "    verbose=1,\r\n",
        "    cv=5,\r\n",
        "    refit=True, return_train_score=True)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "grid_gb.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(grid_gb, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-grid_gb.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(grid_gb.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-VuOWEV4ZGM"
      },
      "source": [
        "'''\r\n",
        "The training RMSE is: 0.1195\r\n",
        "The testing RMSE is: 0.1261\r\n",
        "GradientBoostingRegressor(alpha=0.7, ccp_alpha=0.0, criterion='friedman_mse',\r\n",
        "                          init=None, learning_rate=0.08, loss='ls', max_depth=1,\r\n",
        "                          max_features=None, max_leaf_nodes=None,\r\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\r\n",
        "                          min_samples_leaf=10, min_samples_split=25,\r\n",
        "                          min_weight_fraction_leaf=0.0, n_estimators=2000,\r\n",
        "                          n_iter_no_change=None, presort='deprecated',\r\n",
        "                          random_state=29, subsample=1.0, tol=0.0001,\r\n",
        "                          validation_fraction=0.1, verbose=0, warm_start=False)\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFtAuC4gQK2b"
      },
      "source": [
        "Although my alpha and n_estimators changed, the overall testing RMSE did not change much. I'm going to leave this alone and move on to the next model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSZNXXPEVDH9"
      },
      "source": [
        "Next, I will work on Random Forest. Here are some resources for hyperparameter tuning:\r\n",
        "\r\n",
        "1. [Beginners Guide](https://www.analyticsvidhya.com/blog/2020/03/beginners-guide-random-forest-hyperparameter-tuning/)\r\n",
        "\r\n",
        "2. [Another One](https://medium.com/@ODSC/optimizing-hyperparameters-for-random-forest-algorithms-in-scikit-learn-d60b7aa07ead)\r\n",
        "\r\n",
        "3. [A Good One](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAppJWRuVNdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5cdb3e-58a8-46ce-daff-c4c44112498d"
      },
      "source": [
        "# Create the parameter grid\r\n",
        "param_grid = {\r\n",
        "              'max_depth': range(1, 11),\r\n",
        "              'max_features': ('auto', 'sqrt', 'log2'),\r\n",
        "              'max_leaf_nodes': (20, 40, 60, 80),\r\n",
        "              'min_samples_leaf': range(1, 110, 10),\r\n",
        "              'min_samples_split': (2, 10, 30, 50, 100),\r\n",
        "              'n_estimators': (50, 100, 150, 200),\r\n",
        "              'max_samples': (0.1, 0.2, 0.3, 0.4, 0.5)\r\n",
        "} \r\n",
        "\r\n",
        "# Create a GridSearchCV object\r\n",
        "random_rf1 = RandomizedSearchCV(\r\n",
        "    estimator=RandomForestRegressor(random_state=29),\r\n",
        "    param_distributions=param_grid,\r\n",
        "    scoring='neg_mean_squared_error',\r\n",
        "    n_jobs=-1,\r\n",
        "    random_state=29,\r\n",
        "    n_iter=100,\r\n",
        "    verbose=1,\r\n",
        "    cv=5,\r\n",
        "    refit=True, return_train_score=True)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "random_rf1.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(random_rf1, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-random_rf1.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(random_rf1.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    7.2s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   33.4s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.3min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    7.0s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   32.4s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.3min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    7.0s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   32.0s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.3min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    7.0s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   32.2s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.3min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    7.0s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   31.9s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    7.0s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   32.0s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  1.1min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1515\n",
            "The testing RMSE is: 0.1605\n",
            "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=5, max_features='auto', max_leaf_nodes=60,\n",
            "                      max_samples=0.2, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
            "                      random_state=29, verbose=0, warm_start=False)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.3min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDfqHtknUcqK"
      },
      "source": [
        "'''\r\n",
        "The training RMSE is: 0.1515\r\n",
        "The testing RMSE is: 0.1605\r\n",
        "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\r\n",
        "                      max_depth=5, max_features='auto', max_leaf_nodes=60,\r\n",
        "                      max_samples=0.2, min_impurity_decrease=0.0,\r\n",
        "                      min_impurity_split=None, min_samples_leaf=1,\r\n",
        "                      min_samples_split=10, min_weight_fraction_leaf=0.0,\r\n",
        "                      n_estimators=100, n_jobs=None, oob_score=False,\r\n",
        "                      random_state=29, verbose=0, warm_start=False)\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtE9qIcapeVv"
      },
      "source": [
        "I will come back to XGBRegressor as it's going to take time to learn the API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU9NlYKyMtL_",
        "outputId": "9dd263ca-0507-4884-b5be-fc448198a516"
      },
      "source": [
        "# Create the parameter grid\r\n",
        "param_grid = {\r\n",
        "              'max_depth': range(1, 6),\r\n",
        "              'learning_rate': (0.0001, 0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09),\r\n",
        "              'gamma': (0, 0.0001, 0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 1),\r\n",
        "              'subsample': np.linspace(.01, 1, 10),\r\n",
        "              'colsample_bytree': np.linspace(.01, 1, 10),\r\n",
        "              'n_estimators': (500, 1000, 1500, 2000),\r\n",
        "              'max_samples': (0.1, 0.2, 0.3, 0.4, 0.5),\r\n",
        "              'reg_alpha': (0, 0.0001, 0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 1),\r\n",
        "              'reg_lambda': (0, 0.0001, 0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 1)\r\n",
        "} \r\n",
        "\r\n",
        "# Create a GridSearchCV object\r\n",
        "random_xgb = RandomizedSearchCV(\r\n",
        "    estimator=xgb.XGBRegressor(objective='reg:squarederror', random_state=29),\r\n",
        "    param_distributions=param_grid,\r\n",
        "    scoring='neg_mean_squared_error',\r\n",
        "    n_jobs=-1,\r\n",
        "    random_state=29,\r\n",
        "    n_iter=300,\r\n",
        "    verbose=1,\r\n",
        "    cv=5,\r\n",
        "    refit=True, return_train_score=True)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "random_xgb.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(random_xgb, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-random_xgb.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(random_xgb.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   24.7s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  5.8min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed: 10.2min\n",
            "[Parallel(n_jobs=-1)]: Done 1246 tasks      | elapsed: 15.7min\n",
            "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed: 18.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   19.2s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.7min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   20.3s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.7min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   20.3s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.7min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   20.2s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.7min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   20.3s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.7min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: nan\n",
            "The testing RMSE is: 0.1189\n",
            "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "             colsample_bynode=1, colsample_bytree=0.45, gamma=0.03,\n",
            "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
            "             max_depth=3, max_samples=0.2, min_child_weight=1, missing=None,\n",
            "             n_estimators=2000, n_jobs=1, nthread=None,\n",
            "             objective='reg:squarederror', random_state=29, reg_alpha=0.08,\n",
            "             reg_lambda=0.04, scale_pos_weight=1, seed=None, silent=None,\n",
            "             subsample=0.12, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SF8-yDY4N01"
      },
      "source": [
        "'''\r\n",
        "The training RMSE is: nan\r\n",
        "The testing RMSE is: 0.1189\r\n",
        "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\r\n",
        "             colsample_bynode=1, colsample_bytree=0.45, gamma=0.03,\r\n",
        "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\r\n",
        "             max_depth=3, max_samples=0.2, min_child_weight=1, missing=None,\r\n",
        "             n_estimators=2000, n_jobs=1, nthread=None,\r\n",
        "             objective='reg:squarederror', random_state=29, reg_alpha=0.08,\r\n",
        "             reg_lambda=0.04, scale_pos_weight=1, seed=None, silent=None,\r\n",
        "             subsample=0.12, verbosity=1)\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLnqHOEOjxLK"
      },
      "source": [
        "For some reason my training score was \"nan.\" I'll run another random search but tune the hyperparameters down. I also increased n_estimators as RandomSearch chose the high-end of my range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N-2gf6NUg3z",
        "outputId": "cd5e3999-c221-4ff0-99ca-d944eafd8eed"
      },
      "source": [
        "# Create the parameter grid\r\n",
        "param_grid = {\r\n",
        "              'max_depth': range(2, 4),\r\n",
        "              'learning_rate': (0.005, 0.01, 0.15),\r\n",
        "              'gamma': (0.02, 0.03, 0.04),\r\n",
        "              'subsample': (0.05, 0.12, 0.2),\r\n",
        "              'colsample_bytree': (.035, .045, .055),\r\n",
        "              'n_estimators': (2000, 2500, 3000),\r\n",
        "              'max_samples': (0.15, 0.2, 0.25),\r\n",
        "              'reg_alpha': (0.07, 0.08, 0.09, 1, 1.1),\r\n",
        "              'reg_lambda': (0.03, 0.04, 0.05)\r\n",
        "} \r\n",
        "\r\n",
        "# Create a GridSearchCV object\r\n",
        "random_xgb2 = RandomizedSearchCV(\r\n",
        "    estimator=xgb.XGBRegressor(objective='reg:squarederror', random_state=29),\r\n",
        "    param_distributions=param_grid,\r\n",
        "    scoring='neg_mean_squared_error',\r\n",
        "    n_jobs=-1,\r\n",
        "    random_state=29,\r\n",
        "    n_iter=100,\r\n",
        "    verbose=1,\r\n",
        "    cv=5,\r\n",
        "    refit=True, return_train_score=True)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "random_xgb2.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(random_xgb2, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-random_xgb2.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(random_xgb2.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   32.7s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  5.1min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   25.8s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  4.2min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  4.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   25.6s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  4.2min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  4.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   25.7s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  4.2min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  4.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   25.7s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  4.2min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  4.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   25.6s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  4.2min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  4.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ----------------------------------------------------------------------------------------------------------------------------------------\n",
            "The training RMSE is: 0.1198\n",
            "The testing RMSE is: 0.1276\n",
            "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "             colsample_bynode=1, colsample_bytree=0.055, gamma=0.03,\n",
            "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
            "             max_depth=2, max_samples=0.15, min_child_weight=1, missing=None,\n",
            "             n_estimators=3000, n_jobs=1, nthread=None,\n",
            "             objective='reg:squarederror', random_state=29, reg_alpha=0.09,\n",
            "             reg_lambda=0.05, scale_pos_weight=1, seed=None, silent=None,\n",
            "             subsample=0.2, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZglaShKw4JtH"
      },
      "source": [
        "'''\r\n",
        "The training RMSE is: 0.1198\r\n",
        "The testing RMSE is: 0.1276\r\n",
        "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\r\n",
        "             colsample_bynode=1, colsample_bytree=0.055, gamma=0.03,\r\n",
        "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\r\n",
        "             max_depth=2, max_samples=0.15, min_child_weight=1, missing=None,\r\n",
        "             n_estimators=3000, n_jobs=1, nthread=None,\r\n",
        "             objective='reg:squarederror', random_state=29, reg_alpha=0.09,\r\n",
        "             reg_lambda=0.05, scale_pos_weight=1, seed=None, silent=None,\r\n",
        "             subsample=0.2, verbosity=1)\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGrKJeexkIQY"
      },
      "source": [
        "This random search gave me a worse testing score and I assume it overfit the data. This is because my testing score is higher than my training score AND worse than my prior search. I will lessen the amount of \"n_estimators\" and run a grid search for my final tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3XpxMbLho82",
        "outputId": "70fb8b17-c184-4e91-f179-fe093d0c0456"
      },
      "source": [
        "# Create the parameter grid\r\n",
        "param_grid = {\r\n",
        "              'max_depth': (2, 3),\r\n",
        "              'subsample': (0.12, 0.2),\r\n",
        "              'colsample_bytree': (.045, .055),\r\n",
        "              'n_estimators': (1500, 2000),\r\n",
        "              'max_samples': (0.15, 0.2),\r\n",
        "              'reg_alpha': (0.08, 0.09),\r\n",
        "              'reg_lambda': (0.04, 0.05)\r\n",
        "} \r\n",
        "\r\n",
        "# Create a GridSearchCV object\r\n",
        "grid_xgb = GridSearchCV(\r\n",
        "    estimator=xgb.XGBRegressor(objective='reg:squarederror', random_state=29, learning_rate=0.01, \r\n",
        "                               gamma=0.03),\r\n",
        "    param_grid=param_grid,\r\n",
        "    scoring='neg_mean_squared_error',\r\n",
        "    n_jobs=-1,\r\n",
        "    verbose=1,\r\n",
        "    cv=5,\r\n",
        "    refit=True, return_train_score=True)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "grid_xgb.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(grid_xgb, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-grid_xgb.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(grid_xgb.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   20.5s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=-1)]: Done 640 out of 640 | elapsed:  5.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   15.9s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=-1)]: Done 640 out of 640 | elapsed:  4.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   15.8s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=-1)]: Done 640 out of 640 | elapsed:  4.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   15.8s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=-1)]: Done 640 out of 640 | elapsed:  4.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   15.8s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=-1)]: Done 640 out of 640 | elapsed:  4.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   15.8s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=-1)]: Done 640 out of 640 | elapsed:  4.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ----------------------------------------------------------------------------------------------------------------------------------------\n",
            "The training RMSE is: 0.1237\n",
            "The testing RMSE is: 0.1322\n",
            "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "             colsample_bynode=1, colsample_bytree=0.055, gamma=0.03,\n",
            "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
            "             max_depth=2, max_samples=0.15, min_child_weight=1, missing=None,\n",
            "             n_estimators=2000, n_jobs=1, nthread=None,\n",
            "             objective='reg:squarederror', random_state=29, reg_alpha=0.08,\n",
            "             reg_lambda=0.05, scale_pos_weight=1, seed=None, silent=None,\n",
            "             subsample=0.2, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEXFAU6L3-Ks"
      },
      "source": [
        "'''\r\n",
        "The training RMSE is: 0.1237\r\n",
        "The testing RMSE is: 0.1322\r\n",
        "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\r\n",
        "             colsample_bynode=1, colsample_bytree=0.055, gamma=0.03,\r\n",
        "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\r\n",
        "             max_depth=2, max_samples=0.15, min_child_weight=1, missing=None,\r\n",
        "             n_estimators=2000, n_jobs=1, nthread=None,\r\n",
        "             objective='reg:squarederror', random_state=29, reg_alpha=0.08,\r\n",
        "             reg_lambda=0.05, scale_pos_weight=1, seed=None, silent=None,\r\n",
        "             subsample=0.2, verbosity=1)\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEYs3qkNNrR3"
      },
      "source": [
        "This performed worse than my last tuning. I will take the model, with the lowest RMSE, as my test model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaHvKrAycdWT"
      },
      "source": [
        "Next, I will tune LassoRegressor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2UVGxLSiGJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7772197c-f684-4053-ff53-91601461b0d5"
      },
      "source": [
        "# Create the parameter grid\r\n",
        "param_grid = {'alpha': (0.0001, 0.001, 1),\r\n",
        "              'normalize': (True, False)\r\n",
        "} \r\n",
        "\r\n",
        "# Create a GridSearchCV object\r\n",
        "grid_lasso1 = GridSearchCV(\r\n",
        "    estimator=Lasso(random_state=29),\r\n",
        "    param_grid=param_grid,\r\n",
        "    scoring='neg_mean_squared_error',\r\n",
        "    n_jobs=-1,\r\n",
        "    cv=5,\r\n",
        "    verbose=1,\r\n",
        "    refit=True, return_train_score=True)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "grid_lasso1.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(grid_lasso1, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-grid_lasso1.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(grid_lasso1.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  27 out of  30 | elapsed:    0.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "\n",
            " ----------------------------------------------------------------------------------------------------------------------------------------\n",
            "The training RMSE is: 0.1148\n",
            "The testing RMSE is: 0.1200\n",
            "Lasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000,\n",
            "      normalize=False, positive=False, precompute=False, random_state=29,\n",
            "      selection='cyclic', tol=0.0001, warm_start=False)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUGb2f6l4Ckf"
      },
      "source": [
        "'''\r\n",
        "The training RMSE is: 0.1148\r\n",
        "The testing RMSE is: 0.1200\r\n",
        "Lasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000,\r\n",
        "      normalize=False, positive=False, precompute=False, random_state=29,\r\n",
        "      selection='cyclic', tol=0.0001, warm_start=False)\r\n",
        "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.2s finished\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0ufmgGSlBwQ"
      },
      "source": [
        "I set my alpha level to a very low level because I was tuning these hyperparameters last night. Unfortunately, I was running a gridsearch overnight and did not save my notebook. My computer restarted and I lost my progress. I will run another gridsearch tuning the alpha."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nM0PDvklb3N",
        "outputId": "3cc45820-c9cf-4668-f1bd-3608ed56f2a6"
      },
      "source": [
        "# Create the parameter grid\r\n",
        "param_grid = {'alpha': (0.00001, 0.00005, 0.0001, 0.0002)\r\n",
        "} \r\n",
        "\r\n",
        "# Create a GridSearchCV object\r\n",
        "grid_lasso2 = GridSearchCV(\r\n",
        "    estimator=Lasso(random_state=29),\r\n",
        "    param_grid=param_grid,\r\n",
        "    scoring='neg_mean_squared_error',\r\n",
        "    n_jobs=-1,\r\n",
        "    cv=5,\r\n",
        "    verbose=1,\r\n",
        "    refit=True, return_train_score=True)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "grid_lasso2.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(grid_lasso2, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-grid_lasso2.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(grid_lasso2.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "\n",
            " ----------------------------------------------------------------------------------------------------------------------------------------\n",
            "The training RMSE is: 0.1149\n",
            "The testing RMSE is: 0.1211\n",
            "Lasso(alpha=0.0002, copy_X=True, fit_intercept=True, max_iter=1000,\n",
            "      normalize=False, positive=False, precompute=False, random_state=29,\n",
            "      selection='cyclic', tol=0.0001, warm_start=False)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm7qlwSm8tgX"
      },
      "source": [
        "'''\r\n",
        "The training RMSE is: 0.1149\r\n",
        "The testing RMSE is: 0.1211\r\n",
        "Lasso(alpha=0.0002, copy_X=True, fit_intercept=True, max_iter=1000,\r\n",
        "      normalize=False, positive=False, precompute=False, random_state=29,\r\n",
        "      selection='cyclic', tol=0.0001, warm_start=False)\r\n",
        "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\r\n",
        "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVU52bRr8wUJ"
      },
      "source": [
        "This new model got a different alpha, but also a worse score. I will use the prior one for Lasso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiJOJlamcbuL"
      },
      "source": [
        "Next, I will work on ElasticNet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGkYdZj2YeoQ",
        "outputId": "a77f88a2-3ac6-4e19-b4d1-2ed9a3703ac7"
      },
      "source": [
        "# Create the parameter grid\r\n",
        "param_grid = {'alpha': (0.0001, 0.001, 1),\r\n",
        "              'normalize': (True, False),\r\n",
        "              'l1_ratio': np.linspace(0, 1, 11)\r\n",
        "\r\n",
        "} \r\n",
        "\r\n",
        "# Create a GridSearchCV object\r\n",
        "grid_elasticnet1 = GridSearchCV(\r\n",
        "    estimator=ElasticNet(normalize=True, random_state=29),\r\n",
        "    param_grid=param_grid,\r\n",
        "    scoring='neg_mean_squared_error',\r\n",
        "    n_jobs=-1,\r\n",
        "    cv=5,\r\n",
        "    verbose=1,\r\n",
        "    refit=True, return_train_score=True)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "grid_elasticnet1.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(grid_elasticnet1, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-grid_elasticnet1.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(grid_elasticnet1.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 66 candidates, totalling 330 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 330 out of 330 | elapsed:    2.4s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 66 candidates, totalling 330 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 330 out of 330 | elapsed:    2.0s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 66 candidates, totalling 330 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 330 out of 330 | elapsed:    2.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 66 candidates, totalling 330 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 330 out of 330 | elapsed:    2.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 66 candidates, totalling 330 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 330 out of 330 | elapsed:    2.0s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 66 candidates, totalling 330 fits\n",
            "\n",
            " ----------------------------------------------------------------------------------------------------------------------------------------\n",
            "The training RMSE is: 0.1151\n",
            "The testing RMSE is: 0.1215\n",
            "ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0.1,\n",
            "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "           random_state=29, selection='cyclic', tol=0.0001, warm_start=False)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 300 tasks      | elapsed:    2.9s\n",
            "[Parallel(n_jobs=-1)]: Done 330 out of 330 | elapsed:    3.0s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhB9-ZsT9IxA"
      },
      "source": [
        "'''\r\n",
        "The training RMSE is: 0.1151\r\n",
        "The testing RMSE is: 0.1215\r\n",
        "ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0.1,\r\n",
        "           max_iter=1000, normalize=False, positive=False, precompute=False,\r\n",
        "           random_state=29, selection='cyclic', tol=0.0001, warm_start=False)\r\n",
        "[Parallel(n_jobs=-1)]: Done 300 tasks      | elapsed:    2.9s\r\n",
        "[Parallel(n_jobs=-1)]: Done 330 out of 330 | elapsed:    3.0s finished\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsyiQt639MYg"
      },
      "source": [
        "This is my score for ElasticNet. I'm confident in their values and won't tune further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX3jgZRDdam6"
      },
      "source": [
        "Next I will work on AdaBoostRegressor. Because it is an ensemble metamodel built upon decision trees, I want to tune the base learner. Here are some helpful links:\r\n",
        "\r\n",
        "1. [Tuning cheat sheet](https://medium.com/swlh/the-hyperparameter-cheat-sheet-770f1fed32ff) \r\n",
        "\r\n",
        "2. [Tuning base estimator](https://stackoverflow.com/questions/32210569/using-gridsearchcv-with-adaboost-and-decisiontreeclassifier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1dtnV0idcLP",
        "outputId": "3a38f4b8-2c0b-4fec-cd99-346b53da2062"
      },
      "source": [
        "# Create the parameter grid\r\n",
        "param_grid = {\r\n",
        "              'base_estimator__max_depth': range(1, 4),\r\n",
        "              'learning_rate': (0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1),\r\n",
        "              'n_estimators': (30, 50, 100, 200, 250, 300)\r\n",
        "} \r\n",
        "\r\n",
        "dt = DecisionTreeRegressor()\r\n",
        "# Create a GridSearchCV object\r\n",
        "random_ada = RandomizedSearchCV(\r\n",
        "    estimator=AdaBoostRegressor(base_estimator= dt, random_state=29),\r\n",
        "    param_distributions=param_grid,\r\n",
        "    scoring='neg_mean_squared_error',\r\n",
        "    n_jobs=-1,\r\n",
        "    random_state=29,\r\n",
        "    n_iter=100,\r\n",
        "    verbose=1,\r\n",
        "    cv=5,\r\n",
        "    refit=True, return_train_score=True)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "random_ada.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(random_ada, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-random_ada.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(random_ada.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   18.3s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   14.6s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   15.0s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   14.6s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   15.1s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   14.4s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ----------------------------------------------------------------------------------------------------------------------------------------\n",
            "The training RMSE is: 0.1683\n",
            "The testing RMSE is: 0.1732\n",
            "AdaBoostRegressor(base_estimator=DecisionTreeRegressor(ccp_alpha=0.0,\n",
            "                                                       criterion='mse',\n",
            "                                                       max_depth=3,\n",
            "                                                       max_features=None,\n",
            "                                                       max_leaf_nodes=None,\n",
            "                                                       min_impurity_decrease=0.0,\n",
            "                                                       min_impurity_split=None,\n",
            "                                                       min_samples_leaf=1,\n",
            "                                                       min_samples_split=2,\n",
            "                                                       min_weight_fraction_leaf=0.0,\n",
            "                                                       presort='deprecated',\n",
            "                                                       random_state=None,\n",
            "                                                       splitter='best'),\n",
            "                  learning_rate=0.1, loss='linear', n_estimators=200,\n",
            "                  random_state=29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEpSyBj6J1R0"
      },
      "source": [
        "'''\r\n",
        "The training RMSE is: 0.1683\r\n",
        "The testing RMSE is: 0.1732\r\n",
        "AdaBoostRegressor(base_estimator=DecisionTreeRegressor(ccp_alpha=0.0,\r\n",
        "                                                       criterion='mse',\r\n",
        "                                                       max_depth=3,\r\n",
        "                                                       max_features=None,\r\n",
        "                                                       max_leaf_nodes=None,\r\n",
        "                                                       min_impurity_decrease=0.0,\r\n",
        "                                                       min_impurity_split=None,\r\n",
        "                                                       min_samples_leaf=1,\r\n",
        "                                                       min_samples_split=2,\r\n",
        "                                                       min_weight_fraction_leaf=0.0,\r\n",
        "                                                       presort='deprecated',\r\n",
        "                                                       random_state=None,\r\n",
        "                                                       splitter='best'),\r\n",
        "                  learning_rate=0.1, loss='linear', n_estimators=200,\r\n",
        "                  random_state=29)\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3raEDYOIEAB"
      },
      "source": [
        "This performed marginally better than my base AdaBoost model. On further inspection, I see that there are more base hyperparameters that I could tune. I will tune those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dIXMSgWG2bb",
        "outputId": "e44f2127-7162-464a-ec59-dba5a8d56d4e"
      },
      "source": [
        "# Create the parameter grid\r\n",
        "param_grid = {\r\n",
        "              'learning_rate': (0, 0.01, 0.1, 0.2),\r\n",
        "              'base_estimator__max_features': ('auto', 'sqrt', 'log2'),              \r\n",
        "} \r\n",
        "\r\n",
        "dt = DecisionTreeRegressor(max_depth=3)\r\n",
        "# Create a GridSearchCV object\r\n",
        "grid_ada = GridSearchCV(\r\n",
        "    estimator=AdaBoostRegressor(base_estimator= dt, random_state=29, n_estimators=200),\r\n",
        "    param_grid=param_grid,\r\n",
        "    scoring='neg_mean_squared_error',\r\n",
        "    n_jobs=-1,\r\n",
        "    verbose=1,\r\n",
        "    cv=5,\r\n",
        "    refit=True, return_train_score=True)\r\n",
        "\r\n",
        "# Fit to the training data\r\n",
        "grid_ada.fit(X_train, Y_train)\r\n",
        "\r\n",
        "training_results = np.sqrt((-cross_val_score(grid_ada, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-grid_ada.predict(X_test))**2).mean())\r\n",
        "print('\\n', '-' * 136)\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))\r\n",
        "print(grid_ada.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   16.5s\n",
            "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   21.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:   16.5s\n",
            "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   18.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:   16.7s\n",
            "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   18.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed:   16.6s\n",
            "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   18.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:   16.1s\n",
            "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   17.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:   16.1s\n",
            "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   17.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ----------------------------------------------------------------------------------------------------------------------------------------\n",
            "The training RMSE is: 0.1657\n",
            "The testing RMSE is: 0.1703\n",
            "AdaBoostRegressor(base_estimator=DecisionTreeRegressor(ccp_alpha=0.0,\n",
            "                                                       criterion='mse',\n",
            "                                                       max_depth=3,\n",
            "                                                       max_features='log2',\n",
            "                                                       max_leaf_nodes=None,\n",
            "                                                       min_impurity_decrease=0.0,\n",
            "                                                       min_impurity_split=None,\n",
            "                                                       min_samples_leaf=1,\n",
            "                                                       min_samples_split=2,\n",
            "                                                       min_weight_fraction_leaf=0.0,\n",
            "                                                       presort='deprecated',\n",
            "                                                       random_state=None,\n",
            "                                                       splitter='best'),\n",
            "                  learning_rate=0.2, loss='linear', n_estimators=200,\n",
            "                  random_state=29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI6XORo8JgPJ"
      },
      "source": [
        "'''\r\n",
        "The training RMSE is: 0.1657\r\n",
        "The testing RMSE is: 0.1703\r\n",
        "AdaBoostRegressor(base_estimator=DecisionTreeRegressor(ccp_alpha=0.0,\r\n",
        "                                                       criterion='mse',\r\n",
        "                                                       max_depth=3,\r\n",
        "                                                       max_features='log2',\r\n",
        "                                                       max_leaf_nodes=None,\r\n",
        "                                                       min_impurity_decrease=0.0,\r\n",
        "                                                       min_impurity_split=None,\r\n",
        "                                                       min_samples_leaf=1,\r\n",
        "                                                       min_samples_split=2,\r\n",
        "                                                       min_weight_fraction_leaf=0.0,\r\n",
        "                                                       presort='deprecated',\r\n",
        "                                                       random_state=None,\r\n",
        "                                                       splitter='best'),\r\n",
        "                  learning_rate=0.2, loss='linear', n_estimators=200,\r\n",
        "                  random_state=29)\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5b968c6c-ecc4-44cf-b8ab-57428eba13fc",
        "_uuid": "dc59c17eda8ea78a8de0bba21693086d357e2a2a",
        "id": "U0zZE55Nw0-d"
      },
      "source": [
        "Linear Regression and Ridge Regression have the lowest RMSE. I wonder why a simple linear regression scores so high as compared to the other models.\r\n",
        "\r\n",
        "Next, I will compare the average of the Test RMSE's for my seven models.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6gMnS6s6d2S"
      },
      "source": [
        "Next, I will try two different models with the XGBoost. XGBoost works with SciKit Learn's API when using it's default regressor. It is based on trees as simple learners. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvRsuOkIa2Mo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f61d08-b840-4cd3-e7ad-d91973f5b1cf"
      },
      "source": [
        "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, seed=29, verbose=1, random_state=29)\r\n",
        "xg_reg.fit(X_train, Y_train)\r\n",
        "training_results = np.sqrt((-cross_val_score(xg_reg, X_train, Y_train, cv = 5, scoring= 'neg_mean_squared_error')).mean())\r\n",
        "test_results = np.sqrt(((Y_test-xg_reg.predict(X_test))**2).mean())\r\n",
        "print('The training RMSE is: {:.4f}'.format(training_results))\r\n",
        "print('The testing RMSE is: {:.4f}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training RMSE is: 0.1268\n",
            "The testing RMSE is: 0.1327\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}